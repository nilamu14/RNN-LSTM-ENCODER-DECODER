{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debd8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM , Embedding, BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Activation, Input,Dropout\n",
    "seqnc_length = 10000\n",
    "embedding_dim = 64\n",
    "vocab_size = 10000\n",
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=vocab_size,skip_top=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40cd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d972340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a list of maximum indexes in every review --- we search the maximum index in this list of max indexes\n",
    "print(type([max(sequence) for sequence in x_train]))\n",
    "\n",
    "# Find the maximum of all max indexes\n",
    "max([max(sequence) for sequence in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a957a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's quickly decode a review\n",
    "\n",
    "# step 1: load the dictionary mappings from word to integer index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# step 2: reverse word index to map integer indexes to their respective words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Step 3: decode the review, mapping integer indices to words\n",
    "#\n",
    "# indices are off by 3 because 0, 1, and 2 are reserverd indices for \"padding\", \"Start of sequence\" and \"unknown\"\n",
    "decoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in x_train[0]])\n",
    "\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95e8ff",
   "metadata": {},
   "source": [
    "We cannot feed list of integers into our deep neural network. We will need to convert them into tensors.\n",
    "\n",
    "To prepare our data we will One-hot Encode our lists and turn them into vectors of 0's and 1's. This would blow up all of our sequences into 10,000 dimensional vectors containing 1 at all indices corresponding to integers present in that sequence. This vector will have the element 0 at all indices which are not present in integer sequence.\n",
    "\n",
    "Simply put, the 10,000 dimensional vector corresponding to each review, will have\n",
    "\n",
    "Every index corresponding to a word Every index vith value 1, is a word which is present in the review and is denoted by its integer counterpart Every index containing 0, is a word not present in the review We will vectorize our data manually for maximum clarity. This will result in a tensors of shape (25000, 10000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7a1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "  results = np.zeros((len(sequences), dimension))\n",
    "  for i,sequence in enumerate(sequences):\n",
    "    results[i,sequence] = 1 \n",
    "  return results\n",
    "# Vectorize training Data\n",
    "x_train = vectorize_sequences(x_train)\n",
    "\n",
    "# Vectorize testing Data\n",
    "x_test = vectorize_sequences(x_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a811724",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_test  = np.asarray(y_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c2bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_vec = Input(shape=(seqnc_length,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ec3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import Embedding, LSTM\n",
    "from tensorflow.keras.layers import RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba18400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Encoder network\n",
    "encoded = Dense(100, activation ='tanh',\n",
    "                activity_regularizer = regularizers.l1(10e-5))(inpt_vec)\n",
    "encoded = Dense(50, activation ='tanh',\n",
    "                activity_regularizer = regularizers.l1(10e-5))(encoded)\n",
    "encoded = Dense(25, activation ='tanh',\n",
    "                activity_regularizer = regularizers.l1(10e-5))(encoded)\n",
    "encoded = Dense(12, activation ='tanh',\n",
    "                activity_regularizer = regularizers.l1(10e-5))(encoded)\n",
    "encoded = Dense(6, activation ='relu')(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf01301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Decoder network\n",
    "decoded = Dense(12, activation ='tanh')(encoded)\n",
    "decoded = Dense(25, activation ='tanh')(decoded)\n",
    "decoded = Dense(50, activation ='tanh')(decoded)\n",
    "decoded = Dense(100, activation ='tanh')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Output Layer\n",
    "output_layer = Dense(seqnc_length, activation ='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cadb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(inpt_vec, output_layer)\n",
    "autoencoder.compile(optimizer =\"rmsprop\", loss =\"mse\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd73584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "#callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_delta=1e-4, mode='min', verbose=1)\n",
    "stop_alg = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b05a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = autoencoder.fit(x_train, y_train, batch_size=100, epochs=100,callbacks=[stop_alg, reduce_lr], shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ee7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = autoencoder.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and plot training process\n",
    "autoencoder.save_weights(\"autoencoder.hdf5\")\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.plot(hist.history['loss'], color='#785ef0')\n",
    "plt.plot(hist.history['val_loss'], color='#dc267f')\n",
    "plt.title('Model Loss Progress')\n",
    "plt.ylabel('Brinary Cross-Entropy Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Set', 'Test Set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_hat = autoencoder.predict(x_test)\n",
    "\n",
    "# gets the ROC\n",
    "##fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "##roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plots ROC\n",
    "## fig = plt.figure(figsize=(10,6))\n",
    "## plt.plot(fpr, tpr, color='#785ef0', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "## plt.plot([0, 1], [0, 1], color='#dc267f', linestyle='--')\n",
    "## plt.xlim([0.0, 1.0])\n",
    "## plt.ylim([0.0, 1.05])\n",
    "## plt.xlabel('False Positive Rate')\n",
    "## plt.ylabel('True Positive Rate')\n",
    "## plt.title('Receiver Operating Characteristic Curve')\n",
    "## plt.legend(loc=\"lower right\")\n",
    "## plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224bd134",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros(len(y_hat))\n",
    "for i, score in enumerate(y_hat):\n",
    "    y_pred[i] = np.array([score])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
